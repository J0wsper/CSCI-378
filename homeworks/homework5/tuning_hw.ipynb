{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41df66f0-9815-4dce-bdc7-41896a98a43a",
   "metadata": {},
   "source": [
    "# Tuning\n",
    "\n",
    "In this homework, you'll tune some hyperparameters to try to improve the performance of a network. This will involve setting up a convolutional network for the CIFAR-10 dataset, setting up TensorBoard for logging, and then experimenting. I am _not_ going to be setting specific accuracy targets for different grades because there is too much randomness in the training process. Moreover, achieving high accuracy is easier if you have access to a lot of computational resources, so there are some equity issues with just grading by final model performance.\n",
    "\n",
    "Instead, I'm going to ask you to explain your tuning _process_. That is, for each experiment you run (each set of hyperparameters you try), explain why you ran that experiment and what happened. Based on your observations, what changes did you make for the next run? As long as you have explained your reasoning and it corresponds to the principles we've talked about in class, you'll do fine. Be sure to set up your logging in a way that indicates the hyperparameter values used for each run.\n",
    "\n",
    "**IMPORTANT: Please zip up and submit your TensorBoard log files with your homework. That will help me to see what you were looking at as you went through your tuning process.**\n",
    "\n",
    "I'm also deliberately giving you no starter code for this homework. I understand that a lot of it will just be copy-paste from past classes/labs/homeworks but I still think there is some value in going from a blank document to a complete program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e96ea",
   "metadata": {},
   "source": [
    "## My Program\n",
    "Well, any good machine learning Jupyter notebook starts with a gajillion imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42258a4e-c217-46e7-a969-30635cb8d3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torch.utils.tensorboard as tb\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineRenderer.figure_format = 'retina'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "log_dir = 'homework5_logs'\n",
    "data_dir = '../scratch/data/torch/cifar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da7f4af",
   "metadata": {},
   "source": [
    "## Data\n",
    "The next thing we want to do is important and sanitize our data as well as set up our transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc4c78b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ConvertImageDtype(),\n",
    "])\n",
    "\n",
    "cifar = torchvision.datasets.CIFAR10(data_dir, download=True, transform=transform)\n",
    "train_size = int(0.8 * len(cifar))\n",
    "train_data, valid_data = torch.utils.data.random_split(cifar, [train_size, len(cifar) - train_size])\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(len(cifar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4255822a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4914, 0.4822, 0.4465]) tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "mean = []\n",
    "for x, _ in cifar:\n",
    "    mean.append(torch.mean(x, dim=(1, 2)))\n",
    "mean = torch.stack(mean, dim=0).mean(dim=0)\n",
    "std = []\n",
    "for x, _ in cifar:\n",
    "    std.append(((x - mean[:,np.newaxis,np.newaxis]) ** 2).mean(dim=(1, 2)))\n",
    "std = torch.stack(std, dim=0).mean(dim=0).sqrt()\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22128b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar_std = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "normalize = transforms.Normalize(cifar_mean, cifar_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e4de5",
   "metadata": {},
   "source": [
    "## Setting up our Classes\n",
    "Now it's time to set up our CNN classes. This will look very similar to what I did in homework4, although we'll now be tuning some hyperparameters to get better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35718b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d7e7fc",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "We also want to define a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33d681f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name=\"\", model_class=CNN, lr=1e-3, epochs=10, batch_size=64, momentum=0.9, weight_decay=0):\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(device)\n",
    "\n",
    "    network = model_class().to(device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    name = model_name + '-' + \"homework5-cnn\"\n",
    "    name += '-lr-' + str(lr) + '-bs-' + str(batch_size) + '-mom-' + str(momentum) + '-wght-' + str(weight_decay)\n",
    "    logger = tb.SummaryWriter(os.path.join(log_dir, name))\n",
    "    global_step = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        network.train()\n",
    "        for batch_xs, batch_ys in data_loader:\n",
    "\n",
    "            batch_xs = batch_xs.to(device)\n",
    "            batch_ys = batch_ys.to(device)\n",
    "            batch_xs = normalize(batch_xs)\n",
    "\n",
    "            preds = network(batch_xs)\n",
    "            loss_val = loss(preds, batch_ys)\n",
    "            opt.zero_grad()\n",
    "            loss_val.backward()\n",
    "            opt.step()\n",
    "\n",
    "            logger.add_scalar('loss', loss_val, global_step=global_step)\n",
    "            logger.add_scalar('training accuracy', (preds.argmax(dim=1) == batch_ys).float().mean(), global_step=global_step)\n",
    "\n",
    "            global_step += 1\n",
    "        \n",
    "        network.eval()\n",
    "        accs = []\n",
    "        for batch_xs, batch_ys in valid_loader:\n",
    "\n",
    "            batch_xs = batch_xs.to(device)\n",
    "            batch_ys = batch_ys.to(device)\n",
    "            batch_xs = normalize(batch_xs)\n",
    "\n",
    "            preds = network(batch_xs)\n",
    "            accs.append((preds.argmax(dim=1) == batch_ys).float().mean())\n",
    "\n",
    "        logger.add_scalar('validation accuracy', torch.tensor(accs).mean(), global_step=global_step)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ad12ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# FIRST BATCH\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "cnn_model1 = train(model_name=\"cnn_model1\")\n",
    "cnn_model2 = train(model_name=\"cnn_model2\", lr=5e-3)\n",
    "cnn_model3 = train(model_name=\"cnn_model3\", lr=1e-2)\n",
    "cnn_model4 = train(model_name=\"cnn_model4\", momentum=0.8)\n",
    "cnn_model5 = train(model_name=\"cnn_model5\", lr=5e-3, momentum=0.8)\n",
    "cnn_model6 = train(model_name=\"cnn_model6\", lr=1e-2, momentum=0.8)\n",
    "cnn_model7 = train(model_name=\"cnn_model7\", weight_decay=5e-3)\n",
    "cnn_model8 = train(model_name=\"cnn_model8\", lr=5e-3, weight_decay=5e-3)\n",
    "cnn_model9 = train(model_name=\"cnn_model9\",lr=1e-2, weight_decay=5e-3)\n",
    "\n",
    "torch.save(cnn_model1.state_dict, \"homework5_models/cnn_model1.pt\")\n",
    "torch.save(cnn_model2.state_dict, \"homework5_models/cnn_model2.pt\")\n",
    "torch.save(cnn_model3.state_dict, \"homework5_models/cnn_model3.pt\")\n",
    "torch.save(cnn_model4.state_dict, \"homework5_models/cnn_model4.pt\")\n",
    "torch.save(cnn_model5.state_dict, \"homework5_models/cnn_model5.pt\")\n",
    "torch.save(cnn_model6.state_dict, \"homework5_models/cnn_model6.pt\")\n",
    "torch.save(cnn_model7.state_dict, \"homework5_models/cnn_model7.pt\")\n",
    "torch.save(cnn_model8.state_dict, \"homework5_models/cnn_model8.pt\")\n",
    "torch.save(cnn_model9.state_dict, \"homework5_models/cnn_model9.pt\")\n",
    "\n",
    "# SECOND BATCH\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "cnn_model10 = train(model_name=\"cnn_model10\",lr=5e-3, momentum=0.85)\n",
    "cnn_model11 = train(model_name=\"cnn_model11\",lr=1e-2, momentum=0.85)\n",
    "cnn_model12 = train(model_name=\"cnn_model12\",lr=3e-2, momentum=0.85)\n",
    "cnn_model13 = train(model_name=\"cnn_model13\",lr=5e-3, momentum=0.75)\n",
    "cnn_model14 = train(model_name=\"cnn_model14\",lr=1e-2, momentum=0.75)\n",
    "cnn_model15 = train(model_name=\"cnn_model15\",lr=3e-2, momentum=0.75)\n",
    "cnn_model16 = train(model_name=\"cnn_model16\",lr=5e-3, momentum=0.85, weight_decay=5e-4)\n",
    "cnn_model17 = train(model_name=\"cnn_model17\",lr=1e-2, momentum=0.85, weight_decay=5e-4)\n",
    "cnn_model18 = train(model_name=\"cnn_model18\",lr=3e-2, momentum=0.85, weight_decay=5e-4)\n",
    "\n",
    "torch.save(cnn_model10.state_dict, \"homework5_models/cnn_model10.pt\")\n",
    "torch.save(cnn_model11.state_dict, \"homework5_models/cnn_model11.pt\")\n",
    "torch.save(cnn_model12.state_dict, \"homework5_models/cnn_model12.pt\")\n",
    "torch.save(cnn_model13.state_dict, \"homework5_models/cnn_model13.pt\")\n",
    "torch.save(cnn_model14.state_dict, \"homework5_models/cnn_model14.pt\")\n",
    "torch.save(cnn_model15.state_dict, \"homework5_models/cnn_model15.pt\")\n",
    "torch.save(cnn_model16.state_dict, \"homework5_models/cnn_model16.pt\")\n",
    "torch.save(cnn_model17.state_dict, \"homework5_models/cnn_model17.pt\")\n",
    "torch.save(cnn_model18.state_dict, \"homework5_models/cnn_model18.pt\")\n",
    "\n",
    "# THIRD BATCH\n",
    "\n",
    "# -------------------------------------------------- \n",
    "\n",
    "cnn_model19 = train(model_name=\"cnn_model19\", lr=1e-2, momentum=0.92)\n",
    "cnn_model20 = train(model_name=\"cnn_model20\", lr=9e-3, momentum=0.92)\n",
    "cnn_model21 = train(model_name=\"cnn_model21\", lr=1.1e-2, momentum=0.92)\n",
    "cnn_model22 = train(model_name=\"cnn_model22\", lr=1e-2, weight_decay=5e-5)\n",
    "cnn_model23 = train(model_name=\"cnn_model23\", lr=5e-3, epochs=15, weight_decay=5e-5)\n",
    "cnn_model24 = train(model_name=\"cnn_model24\", lr=1e-2, epochs=15)\n",
    "\n",
    "torch.save(cnn_model19.state_dict, \"homework5_models/cnn_model19.pt\")\n",
    "torch.save(cnn_model20.state_dict, \"homework5_models/cnn_model20.pt\")\n",
    "torch.save(cnn_model21.state_dict, \"homework5_models/cnn_model21.pt\")\n",
    "torch.save(cnn_model22.state_dict, \"homework5_models/cnn_model22.pt\")\n",
    "torch.save(cnn_model23.state_dict, \"homework5_models/cnn_model23.pt\")\n",
    "torch.save(cnn_model24.state_dict, \"homework5_models/cnn_model24.pt\")\n",
    "\n",
    "# FOURTH BATCH\n",
    "\n",
    "# -------------------------------------------------- \n",
    "\n",
    "cnn_model25 = train(model_name=\"cnn_model25\", lr=5e-3, epochs=20, weight_decay=5e-5)\n",
    "cnn_model26 = train(model_name=\"cnn_model26\", lr=7e-3, epochs=20, weight_decay=5e-5)\n",
    "cnn_model27 = train(model_name=\"cnn_model27\", lr=9e-3, epochs=20, weight_decay=5e-5)\n",
    "\n",
    "torch.save(cnn_model25.state_dict, \"homework5_models/cnn_model25.pt\")\n",
    "torch.save(cnn_model26.state_dict, \"homework5_models/cnn_model26.pt\")\n",
    "torch.save(cnn_model27.state_dict, \"homework5_models/cnn_model27.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c401305a-9b58-45b5-9f65-c09ca08be36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 255).\n",
       "Contents of stderr:\n",
       "TensorFlow installation not found - running with reduced feature set.\n",
       "\n",
       "NOTE: Using experimental fast data loading logic. To disable, pass\n",
       "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
       "    https://github.com/tensorflow/tensorboard/issues/4784\n",
       "\n",
       "E0303 12:47:20.139552 140571579766592 program.py:300] TensorBoard could not bind to port 20005, it was already in use\n",
       "ERROR: TensorBoard could not bind to port 20005, it was already in use"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir={log_dir} --port 20005"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
