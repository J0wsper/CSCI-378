{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82843a3e-0776-4c72-a455-63fbb4dccbe4",
   "metadata": {},
   "source": [
    "# Comparing MLP and Convolutional Models\n",
    "\n",
    "In this week's homework, we'll use PyTorch to compare the performance of multi-layer perceptrons (MLP's; the kind of model we've looked at so far) against _convolutional_ neural networks (CNN's). We'll talk about convolutions in class on Monday, but you can get started on the MLP part of the homework as soon as you're ready. For our comparison we'll go back to the CIFAR-10 dataset, since it's a bit less chaotic than CIFAR-100.\n",
    "\n",
    "As with the last homework, I have some guidelines about what parts of the homework are necessary for different grades:\n",
    "\n",
    "- The basic version, for a C, is to define and train an MLP and a CNN.\n",
    "- On top of that, the B level work requires you to analyze your results a bit. I'll describe this in more detail later in the notebook after the code that sets up and trains the networks.\n",
    "- For an A, you'll need to finish the activation map visualization in the last section of this notebook. More details on that later on.\n",
    "\n",
    "The rest of the document is organized into sections which are labeled with the grade they correspond to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24cd25-bc95-4507-971b-e407c7b8b468",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "The first section of the notebook gets the dataset and sets up the transforms we need. The code in this section is complete, although you may need to change the dataset path or change the `transform` definition to match your version of torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0090403a-c772-4aa2-aa57-484713eaf063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineRenderer.figure_format = 'retina'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed2cc76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbe40599-58d5-4920-9495-13f356e7c91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ConvertImageDtype(),\n",
    "    # Depending on your torchvision version you may need to change these:\n",
    "    # - If you don't have torchvision.transforms.v2, then import torchvision.transforms\n",
    "    #   instead and use ToTensor() to replace _both_ of the transforms above.\n",
    "    # - If you have v2 but it says ToImage() is undefined, then use ToImageTensor() instead.\n",
    "])\n",
    "\n",
    "# If you already have the CIFAR10 data downloaded from the in-class notebook, you can change the path here\n",
    "# to point to it so you avoid downloading a second copy.\n",
    "cifar = torchvision.datasets.CIFAR10(\"../../data/torch/cifar\", download=True, transform=transform)\n",
    "train_size = int(0.8 * len(cifar))\n",
    "train_data, valid_data = torch.utils.data.random_split(cifar, [train_size, len(cifar) - train_size])\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(len(cifar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff2c4238-357f-4932-8be6-189e70fe96c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4914, 0.4822, 0.4465]) tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "mean = []\n",
    "for x, _ in cifar:\n",
    "    mean.append(torch.mean(x, dim=(1, 2)))\n",
    "mean = torch.stack(mean, dim=0).mean(dim=0)\n",
    "std = []\n",
    "for x, _ in cifar:\n",
    "    std.append(((x - mean[:,np.newaxis,np.newaxis]) ** 2).mean(dim=(1, 2)))\n",
    "std = torch.stack(std, dim=0).mean(dim=0).sqrt()\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7fa3b-fe94-4ad5-a11d-0af917329dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar_std = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "normalize = transforms.Normalize(cifar_mean, cifar_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c144a-b2ae-455b-a636-ebd7a28806db",
   "metadata": {},
   "source": [
    "## Models and Training (C)\n",
    "\n",
    "First, define an MLP model for the CIFAR dataset. An MLP, also called a fully-connected network, consists of linear computations alternated with nonlinear activation functions, just like every network we've looked at in this class so far. This is very similar to what we did in clas on Wednesday and lab on Friday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5917c17-dd34-4d98-82aa-47a2734f75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3*32*32, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.reshape(-1, 3*32*32).squeeze(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0cb82-472e-4c23-8e83-b30df497de8a",
   "metadata": {},
   "source": [
    "Now let's define a training function for our MLP. As usual, you may want to add more arguments to the training function. For the latter parts of the notebook, it will be helpful if your training function returns both the model and a list of the training and validation accuracies for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7791e-a629-4776-b259-c475dae86722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_class=MLP, lr=1e-3, epochs=10, batch_size=64):\n",
    " \n",
    "    data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "    print(device)\n",
    "    network = model_class().to(device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), momentum=0.9, lr=lr, weight_decay=0)\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        for batch_xs, batch_ys in data_loader:\n",
    "            batch_xs = batch_xs.to(device)\n",
    "            batch_ys = batch_ys.to(device)\n",
    "            batch_xs = normalize(batch_xs)\n",
    "            preds = network(batch_xs)\n",
    "            loss_val = loss(preds, batch_ys)\n",
    "            opt.zero_grad()\n",
    "            loss_val.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        for batch_xs, batch_ys in valid_loader:\n",
    "\n",
    "            batch_xs = batch_xs.to(device)\n",
    "            batch_ys = batch_ys.to(device)\n",
    "            batch_xs = normalize(batch_xs)\n",
    "            preds = network(batch_xs)\n",
    "            valid_accs.append((preds.argmax(dim=1) == batch_ys).float().mean())\n",
    "        \n",
    "        print(\"Epoch:\", i, \"Validation accuracy:\", torch.tensor(valid_accs).mean().item())\n",
    "\n",
    "    return network, train_accs, valid_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db839eaf-5da8-479b-8280-4b67556bc7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 0 Validation accuracy: 0.4468550980091095\n",
      "Epoch: 1 Validation accuracy: 0.4641222059726715\n",
      "Epoch: 2 Validation accuracy: 0.47438958287239075\n",
      "Epoch: 3 Validation accuracy: 0.4836285710334778\n",
      "Epoch: 4 Validation accuracy: 0.48859474062919617\n",
      "Epoch: 5 Validation accuracy: 0.4947584867477417\n",
      "Epoch: 6 Validation accuracy: 0.498492956161499\n",
      "Epoch: 7 Validation accuracy: 0.5004354119300842\n",
      "Epoch: 8 Validation accuracy: 0.5040804147720337\n",
      "Epoch: 9 Validation accuracy: 0.5054836869239807\n",
      "Epoch: 10 Validation accuracy: 0.5073646306991577\n",
      "Epoch: 11 Validation accuracy: 0.5089486837387085\n",
      "Epoch: 12 Validation accuracy: 0.50960773229599\n",
      "Epoch: 13 Validation accuracy: 0.5101014971733093\n",
      "Epoch: 14 Validation accuracy: 0.5107550621032715\n",
      "CPU times: user 7min 42s, sys: 6.11 s, total: 7min 48s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mlp_model, mlp_train_accs, mlp_valid_accs = train(model_class=MLP, lr=5e-3, epochs=15, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd0d38-aa25-4d97-bd63-0e61c4320448",
   "metadata": {},
   "source": [
    "Now you can define a CNN for the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608fb146-c6eb-404b-9ac5-85c1e35675ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5805ea5f-607d-45b9-b32c-84c6248b7b41",
   "metadata": {},
   "source": [
    "Now you can train your CNN. You should be able use the same `train` function but pass it `model_class=CNN` (you may need to change some other hyperparameters to get good results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded6d3f-5f6f-4a27-8e24-5cc6f770d6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 0 Validation accuracy: 0.5014928579330444\n",
      "Epoch: 1 Validation accuracy: 0.5137838125228882\n",
      "Epoch: 2 Validation accuracy: 0.5341693162918091\n",
      "Epoch: 3 Validation accuracy: 0.5486415028572083\n",
      "Epoch: 4 Validation accuracy: 0.5607882142066956\n",
      "Epoch: 5 Validation accuracy: 0.5697319507598877\n",
      "Epoch: 6 Validation accuracy: 0.5754805207252502\n",
      "Epoch: 7 Validation accuracy: 0.5804264545440674\n",
      "Epoch: 8 Validation accuracy: 0.5858324766159058\n",
      "Epoch: 9 Validation accuracy: 0.5895799994468689\n",
      "Epoch: 10 Validation accuracy: 0.5935328602790833\n",
      "Epoch: 11 Validation accuracy: 0.5956408977508545\n",
      "Epoch: 12 Validation accuracy: 0.598251461982727\n",
      "Epoch: 13 Validation accuracy: 0.6004464030265808\n",
      "Epoch: 14 Validation accuracy: 0.6020634174346924\n",
      "CPU times: user 12min 7s, sys: 7.21 s, total: 12min 14s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cnn_model, cnn_train_accs, cnn_valid_accs = train(model_class=CNN, lr=8e-3, epochs=15, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb01db-f89f-4a53-a5b1-77faf2bd8dc2",
   "metadata": {},
   "source": [
    "Given how long it takes to train the CNN model, now might be a good time to talk about saving and loading models. The cell below will save your CNN model to the file `cnn_model.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b2210-1745-4283-89ce-9bfca9653fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cnn_model.state_dict(), \"cnn_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ea796-3666-4bee-90a7-9c9e32c06b13",
   "metadata": {},
   "source": [
    "If you have the file `cnn_model.pt` on your system, then you can load it with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8208a-8b61-4220-b095-3d56731130be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130800/1420923919.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(\"cnn_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = CNN()\n",
    "loaded_model.load_state_dict(torch.load(\"cnn_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651538a-987a-4d96-afed-c6d38825d0f1",
   "metadata": {},
   "source": [
    "## Analysis (B)\n",
    "\n",
    "In this section, we'll do some minor analysis of the results of our experiment above. Let's compare\n",
    "\n",
    "- the time taken to train each model,\n",
    "- the validation accuracy over time,\n",
    "- the total number of parameters in each model.\n",
    "\n",
    "That last measurement, the total number of parameters, is something you'll need to compute pased on your network architecture. When I say number of parameters here, I'm referring to the total number of dimensions in the parameter space. That is, the total number of individual real numbers in our parameters. For example, a matrix of size 100x10 has 1,000 parameters.\n",
    "\n",
    "I'm not asking for anything specific here, I'm just looking for you to think about the models we're using and their relative merits. If you think of something else that might be useful to compare the two networks, mention that as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27817c9-edfc-4659-b1b5-f3f49b22f2f3",
   "metadata": {},
   "source": [
    "### What Do We Learn?\n",
    "- In terms of the time it takes to train each model, they're roughly comparable at just over 1 minute running on the CUDA-enabled GPUs.\n",
    "- The validation accuracy of the CNN model is significantly higher both initially and over time. I have adjusted the MLP hyperparameters a bit and no matter what I choose, the CNN simply performs better. The validation accuracy of the MLP tops out at roughly \\~51%, while the CNN tops out at over \\~60%. An interesting detail is that with the learning rate I select, both the CNN and the MLP start at roughly the same validation accuracy (~47%) but the CNN plateaus much later than the MLP.\n",
    "- In terms of parameters, I did some math and found that the MLP has 823936 parameters in the formulation I produced while the CNN has 23584 parameters. Therefore, the MLP has over 33 times more parameters than the CNN. A whole order of magnitude more parameters is a pretty bad look unfortunately.\n",
    "\n",
    "### Relative Merits of the Models\n",
    "- In this case, I think that the CNN just had the MLP beat for the most part.\n",
    "- Its reduced number of parameters means that overfitting is less likely (unless there's something else in the CNN formulation that I'm missing that would change this fact).\n",
    "- The CNN also obtains much higher validation accuracy than the MLP even without extensive hyperparameter tweaking.\n",
    "- Finally, the CNN allows for slightly more flexibility in terms of how we want to do training because of the interleaving of `ReLU`, `MaxPool2d` and `AvgPool2d` rather than relying on the traditional activation functions of an MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812b240-336d-4aa2-aee4-0ff9ff13ee11",
   "metadata": {},
   "source": [
    "## Interpretation (A)\n",
    "\n",
    "In this section, we'll explore our CNN and try to understand how it learns to recognize different objects. There are several approaches to this problem. For today we'll look at a simple one that only works for the first convolutional layer. If we plot the weights of the layer as image data, we can visualize the kinds of patterns the convolution is scanning for. In order to do this, you'll need to normalize each kernel to the range [0, 1], then transpose the axis to format the weights as image data, then use the `imshow` function from `matplotlib.pyplot`. The weights of a convolutional layer `conv` are stored as a tensor of shape `Cin, Cout, H, W` in `conv.weight`.\n",
    "\n",
    "If you have a torch tensor `x` and you want to display it with `imshow` you'll need to convert it to numpy by calling `x.detach().numpy()`.\n",
    "\n",
    "Once you've got the plots displaying, take a minute to think about convolutions and try to tell me what the plots mean in terms of how the network recognizes images. (I'm being deliberately a bit vague about this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2fe436-5667-4f21-83a0-3373714c5a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If your first convolutional layer has fewer than 16 output channels then you'll need to\n",
    "# change the number of plots here.\n",
    "\n",
    "# Getting our model layers first and putting them into a dictionary\n",
    "named_module_dict = dict(loaded_model.named_parameters())\n",
    "\n",
    "# Getting the first convolutional layer as a tensor\n",
    "conv = named_module_dict['model.0.weight'].data\n",
    "\n",
    "# Normalizing it so that all values are in [0,1]\n",
    "conv -= conv.min()\n",
    "conv /= conv.max()\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        c = 4 * i + j\n",
    "\n",
    "        axs[i,j].imshow(conv[c].detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33420262",
   "metadata": {},
   "source": [
    "### What This Means\n",
    "- Okay so these are the 16 kernels that the CNN network produces in its first convolutional layer.\n",
    "- A lot of them are more abstract and its hard to tell what exactly they're doing but some of them make sense from a human-readable perspective.\n",
    "- I'm going to number the kernels 1-16 going from top-left to bottom-right for ease of speaking.\n",
    "- Clearly, 12 and 16 are designed to recognize grass and/or natural scenes. Apparently, our model thinks these are important in classifying data because it has two of them that are very similar.\n",
    "- 14 is a very interesting kernel because the bottom half being entirely blue leads me to believe it is designed to recognize either boats or airplanes. 11 is similar but the blue is on top, leading me to suspect that this one is specifically for airplanes.\n",
    "- 9 and 13 are very similar too. I can't tell if they're trying to recognize man-made structures with the grey and brown or something else. Perhaps these are for the dogs and cats?\n",
    "- 10 is a weird one. I have no idea what is going on there or why it produces such clear color delineations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci378-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
