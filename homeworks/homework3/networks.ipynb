{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c0fa9f-55e4-4311-b87f-9fb26c6a8986",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "This homework will be an extension of the lab we did working with CIFAR data (the homework doesn't come bundled with the data since you already have it from lab. If you don't have it you can get it from the lab from this week). We'll convert our network from a binary to a multiclass setting, much like you did with the MNIST data, and we'll experiment with some ways to improve network performance.\n",
    "\n",
    "NOTE: We'll be talking about PyTorch during the week before this homework is due, but _don't_ use PyTorch for this homework. We'll switch over the PyTorch for the next one, but I think it's important to understand how the machine learning algorithms work under the hood.\n",
    "\n",
    "First we'll load and normalize the data, just like we had in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "89097c37-4179-4646-bc02-9e2a4f64aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def load_data(dirname):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(1, 6):\n",
    "        with open(dirname + '/data_batch_' + str(i), 'rb') as fo:\n",
    "            obj = pickle.load(fo, encoding='bytes')\n",
    "            data.append(obj[b'data'])\n",
    "            labels.append(obj[b'labels'])\n",
    "    with open(dirname + '/batches.meta', 'rb') as fo:\n",
    "        names = pickle.load(fo, encoding='bytes')[b'label_names']\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    labels = np.concatenate(labels)\n",
    "    return data.reshape(-1, 3, 32, 32), labels, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "08688f12-b448-4320-a781-fecb0a6d1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to change the path below--it should point to the directory where\n",
    "# the CIFAR data files are held on your system.\n",
    "images, labels, names = load_data('../../data/cifar')\n",
    "permute = np.random.permutation(labels.shape[0])\n",
    "images = images[permute]\n",
    "labels = labels[permute]\n",
    "\n",
    "mean = np.mean(images, axis=(0, 2, 3), keepdims=True)\n",
    "std = np.std(images, axis=(0, 2, 3), keepdims=True)\n",
    "\n",
    "norm_images = (images - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6063de-e74b-41bf-9f2e-b919d27535cb",
   "metadata": {},
   "source": [
    "Now we'll set up and train a neural network to classify these images. There are different options you can choose from at different levels of complexity. The baseline assignment is to define a neural network with at least one hidden layer using ReLU as the activation function. Train your network using the multiclass cross-entropy loss we defined in the previous homework assignment. Completing this part of the assignment (correctly) guarantees at least a C.\n",
    "\n",
    "Beyond this, I have two lists of possible extensions:\n",
    "\n",
    "- Minor\n",
    "  + Add $L_2$ regularization. That is, if your parameters are $\\beta$, minimize $L(\\beta) + \\lambda \\| \\beta \\|_2^2$. Remember that $\\| \\beta \\|_2^2 = \\sum_i \\beta_i^2$. You'll need to compute the gradient of $\\| \\beta \\|_2^2$ and add it to your gradients for each parameter. You'll need to choose a regularization hyperparameter $\\lambda$. I recommend starting around 1e-4. How does this change your results (if at all)? (Note that the regularization in the lab solution is $L_1$ regularization, so this is slightly different.)\n",
    "  + Use a different activation function to replace the ReLU. This will require computing the derivative of your new activation function and replacing the derivative of ReLU with it. (The derivative of ReLU is the line in the lab `h_grad[h <= 0] = 0`. We've already computed the derivative of $\\tanh$ in a previous homework, so that might be a good choice.) How does this change your results (if at all)?\n",
    "  + Split your data into a training set and a validation set. Track and plot the training accuracy and validation accuracy during training. What (if any) information can you get from this plot?\n",
    "- Major\n",
    "  + Define your training function to accept a _list_ of hidden layer sizes and set up the network accordingly. For example, I should be able to pass your function the list `[256, 128]` and it will create a network with two hidden layers of sizes 256 and 128. But if I pass it `[256, 128, 64]` then it should create a network with _three_ hidden layers of appropriate size.\n",
    "  + Use at least two hidden layers and add data augmentation. That is, for each iteration of training, you should randomly perturb each image independently. A couple of good options are listed below. Implement at least two.\n",
    "    * Horizontal flip: randomly mirror an image with probability 0.5\n",
    "    * Grayscale: replace each color value with the average of all three color values in an image with probability 0.5\n",
    "    * Brightness: choose a factor uniformly at random in some range around one (say 0.9-1.1) then multiply each pixel value by that factor. You can also choose different factors for each color channel.\n",
    "    * Shift: randomly choose a number of pixels (our images are small so probably around 3 or 4 at most) and a direction. Shift the image by the chosen number of pixels in the chosen direction and fill in the leftover gap with the mean values of each color.\n",
    "    \n",
    "Using at least two hidden layers and doing any two of the minor extensions _or_ a major extension (correctly) guarantees a B. Doing all of the minor extensions and at least one of the major extensions (correctly) is good for an A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f7653-786b-4bd7-b4f1-114249e9422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to add more arguments to the train function as you go\n",
    "\n",
    "def train_relu(xs, ys, valid_xs, valid_ys, epochs=10, lr=5e-5, batch_size=64, reg=1e-4):\n",
    "    \n",
    "    # print(\"XS shape: \", xs.shape)\n",
    "    # print(\"YS shape: \", ys.shape)\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # Getting our hidden layer sizes\n",
    "    size1 = 512\n",
    "    size2 = 256\n",
    "\n",
    "    # Set up the model\n",
    "\n",
    "    # We go from 512 -> 256 -> 10 which is our final prediction\n",
    "    # This architecture uses 2 hidden layers like in the lab\n",
    "    w1 = rng.normal(scale=np.sqrt(2/(xs.shape[1]+size1)), size=(xs.shape[1], size1))\n",
    "    b1 = np.zeros(size1)\n",
    "    w2 = rng.normal(scale=np.sqrt(2/(size1+size2)), size=(size1, size2))\n",
    "    b2 = np.zeros(size2)\n",
    "    w3 = rng.normal(scale=np.sqrt(2/(size2+10)), size=(size2, 10))\n",
    "    b3 = np.zeros(10)\n",
    "\n",
    "    # print(\"W1 shape: \", w1.shape)\n",
    "    # print(\"B1 shape: \", b1.shape)\n",
    "    # print(\"W2 shape: \", w2.shape)\n",
    "    # print(\"B2 shape: \", b2.shape)\n",
    "    # print(\"W3 shape: \", w3.shape)\n",
    "    # print(\"B3 shape: \", b3.shape)\n",
    "\n",
    "    # We also need the one-hot encoding for the gradient of softmax\n",
    "    one_hot = np.zeros((ys.shape[0], 10))\n",
    "    one_hot[np.arange(ys.shape[0]), ys] = 1\n",
    "    \n",
    "    num_batches = xs.shape[0] // batch_size\n",
    "    if xs.shape[0] % batch_size != 0:\n",
    "        num_batches += 1\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Shuffle the data at the start of each epoch\n",
    "        permute = rng.permutation(xs.shape[0])\n",
    "        xs = xs[permute]\n",
    "        ys = ys[permute]\n",
    "        one_hot = one_hot[permute]\n",
    "        \n",
    "        for j in range(num_batches):\n",
    "\n",
    "            end_index = min(batch_size * (j + 1), xs.shape[0])\n",
    "            batch_xs = xs[batch_size * j : end_index]\n",
    "            batch_ys = ys[batch_size * j : end_index]\n",
    "            batch_one_hot = one_hot[batch_size * j : end_index]\n",
    "            \n",
    "            # Do your loss and gradient computations here\n",
    "\n",
    "            # We first compute the hs with our tanh activation function.\n",
    "            h1 = np.tanh(batch_xs @ w1 + b1)\n",
    "            h2 = np.tanh(h1 @ w2 + b2)\n",
    "            \n",
    "            # We then get the value we're going to plug into the softmax function\n",
    "            logits = (h2 @ w3 + b3).squeeze()\n",
    "            exp_logits = np.exp(logits)\n",
    "\n",
    "            # Applying softmax like we did in HW2 and calculating which ones our model\n",
    "            # got correct\n",
    "            preds = exp_logits/np.expand_dims(np.sum(exp_logits, axis=1), axis=1)\n",
    "            correct_preds = preds[np.arange(preds.shape[0]), batch_ys]\n",
    "\n",
    "            # Calculating loss \n",
    "            loss = np.mean(-np.log(correct_preds))\n",
    "\n",
    "            # Calculating the gradients\n",
    "            logit_grad = (preds-batch_one_hot)[:,:,np.newaxis]\n",
    "            b3_grad = np.sum(logit_grad, axis=0).squeeze()\n",
    "            w3_grad = h2.T @ logit_grad.squeeze()\n",
    "            h2_grad = 1 - np.power(np.tanh(logit_grad.squeeze() @ w3.T), 2) # Change this\n",
    "            b2_grad = np.sum(h2_grad, axis=0)\n",
    "            w2_grad = h1.T @ h2_grad\n",
    "            h1_grad = 1 - np.power(np.tanh(h2_grad @ w2.T), 2) # Change this\n",
    "            h1_grad = h2_grad @ w2.T\n",
    "            b1_grad = np.sum(h1_grad, axis=0)\n",
    "            w1_grad = batch_xs.T @ h1_grad\n",
    "\n",
    "            # d/dx tanh(x) = 1 - tanh^2(x)\n",
    "\n",
    "            # if i == 0 and j == 0:\n",
    "            #     print(\"Batch shape: \", batch_xs.shape)\n",
    "            #     print(\"Preds shape: \", preds.shape)\n",
    "            #     print(\"One hot batch shape: \", batch_one_hot.shape)\n",
    "            #     print(\"Logit shape:  | Logit grad shape: \", logits.shape, logit_grad.shape)\n",
    "            #     print(\"H1 shape: | H1 grad shape: \", h1.shape, h1_grad.shape)\n",
    "            #     print(\"H2 shape: | H2 grad shape: \", h2.shape, h2_grad.shape)\n",
    "            #     print(\"W3 shape: | W3 grad shape: \", w3.shape, w3_grad.shape)\n",
    "            #     print(\"W2 shape: | W2 grad shape: \", w2.shape, w2_grad.shape)\n",
    "            #     print(\"W1 shape: | W1 grad shape: \", w1.shape, w1_grad.shape)\n",
    "            #     print(\"B3 shape: | B3 grad shape: \", b3.shape, b3_grad.shape)\n",
    "            #     print(\"B2 shape: | B2 grad shape: \", b2.shape, b2_grad.shape)\n",
    "            #     print(\"B1 shape: | B1 grad shape: \", b1.shape, b1_grad.shape)\n",
    "\n",
    "            # Applying L2 regularization\n",
    "            w1_grad += reg * np.power(w1, 2)\n",
    "            b1_grad += reg * np.power(b1, 2)\n",
    "            h1_grad += reg * np.power(h1, 2)\n",
    "            w2_grad += reg * np.power(w2, 2)\n",
    "            b2_grad += reg * np.power(b2, 2)\n",
    "            h2_grad += reg * np.power(h2, 2)\n",
    "            w3_grad += reg * np.power(w3, 2)\n",
    "            b3_grad += reg * np.power(b3, 2)\n",
    "\n",
    "            # Modifying our parameters\n",
    "            w1 -= lr * w1_grad\n",
    "            b1 -= lr * b1_grad\n",
    "            h1 -= lr * h1_grad\n",
    "            w2 -= lr * w2_grad\n",
    "            b2 -= lr * b2_grad\n",
    "            h2 -= lr * h2_grad\n",
    "            w3 -= lr * w3_grad\n",
    "            b3 -= lr * b3_grad\n",
    "           \n",
    "        # Testing our validation accuracy \n",
    "        h1 = np.tanh(valid_xs @ w1 + b1)\n",
    "        h2 = np.tanh(h1 @ w2 + b2)\n",
    "        logits = (h2 @ w3 + b3).squeeze()\n",
    "        preds = 1 / (1 + np.exp(-logits))\n",
    "        preds_max = np.argmax(preds, axis=1)\n",
    "        accuracy = np.mean(preds_max == valid_ys)\n",
    "        print(\"Epoch:\", i, \"Validation accuracy:\", accuracy)\n",
    "        \n",
    "def train_tanh(xs, ys, valid_xs, valid_ys, epochs=10, lr=5e-5, batch_size=64, reg=1e-4):\n",
    "    \n",
    "    # print(\"XS shape: \", xs.shape)\n",
    "    # print(\"YS shape: \", ys.shape)\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # Getting our hidden layer sizes\n",
    "    size1 = 512\n",
    "    size2 = 256\n",
    "\n",
    "    # Set up the model\n",
    "\n",
    "    # We go from 512 -> 256 -> 10 which is our final prediction\n",
    "    # This architecture uses 2 hidden layers like in the lab\n",
    "    w1 = rng.normal(scale=np.sqrt(2/(xs.shape[1]+size1)), size=(xs.shape[1], size1))\n",
    "    b1 = np.zeros(size1)\n",
    "    w2 = rng.normal(scale=np.sqrt(2/(size1+size2)), size=(size1, size2))\n",
    "    b2 = np.zeros(size2)\n",
    "    w3 = rng.normal(scale=np.sqrt(2/(size2+10)), size=(size2, 10))\n",
    "    b3 = np.zeros(10)\n",
    "\n",
    "    # print(\"W1 shape: \", w1.shape)\n",
    "    # print(\"B1 shape: \", b1.shape)\n",
    "    # print(\"W2 shape: \", w2.shape)\n",
    "    # print(\"B2 shape: \", b2.shape)\n",
    "    # print(\"W3 shape: \", w3.shape)\n",
    "    # print(\"B3 shape: \", b3.shape)\n",
    "\n",
    "    # We also need the one-hot encoding for the gradient of softmax\n",
    "    one_hot = np.zeros((ys.shape[0], 10))\n",
    "    one_hot[np.arange(ys.shape[0]), ys] = 1\n",
    "    \n",
    "    num_batches = xs.shape[0] // batch_size\n",
    "    if xs.shape[0] % batch_size != 0:\n",
    "        num_batches += 1\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Shuffle the data at the start of each epoch\n",
    "        permute = rng.permutation(xs.shape[0])\n",
    "        xs = xs[permute]\n",
    "        ys = ys[permute]\n",
    "        one_hot = one_hot[permute]\n",
    "        \n",
    "        for j in range(num_batches):\n",
    "\n",
    "            end_index = min(batch_size * (j + 1), xs.shape[0])\n",
    "            batch_xs = xs[batch_size * j : end_index]\n",
    "            batch_ys = ys[batch_size * j : end_index]\n",
    "            batch_one_hot = one_hot[batch_size * j : end_index]\n",
    "            \n",
    "            # Do your loss and gradient computations here\n",
    "\n",
    "            # We first compute the hs with our tanh activation function.\n",
    "            h1 = np.maximum(batch_xs @ w1 + b1, 0)\n",
    "            h2 = np.maximum(h1 @ w2 + b2, 0)\n",
    "            # h1 = np.tanh(batch_xs @ w1 + b1)\n",
    "            # h2 = np.tanh(h1 @ w2 + b2)\n",
    "            \n",
    "            # We then get the value we're going to plug into the softmax function\n",
    "            logits = (h2 @ w3 + b3).squeeze()\n",
    "            exp_logits = np.exp(logits)\n",
    "\n",
    "            # Applying softmax like we did in HW2 and calculating which ones our model\n",
    "            # got correct\n",
    "            preds = exp_logits/np.expand_dims(np.sum(exp_logits, axis=1), axis=1)\n",
    "            correct_preds = preds[np.arange(preds.shape[0]), batch_ys]\n",
    "\n",
    "            # Calculating loss \n",
    "            loss = np.mean(-np.log(correct_preds))\n",
    "\n",
    "            # Calculating the gradients\n",
    "            logit_grad = (preds-batch_one_hot)[:,:,np.newaxis]\n",
    "            b3_grad = np.sum(logit_grad, axis=0).squeeze()\n",
    "            w3_grad = h2.T @ logit_grad.squeeze()\n",
    "            # h2_grad = 1 - np.power(np.tanh(logit_grad.squeeze() @ w3.T), 2) # Change this\n",
    "            h2_grad = logit_grad.squeeze() @ w3.T\n",
    "            h2_grad[h2 <= 0] = 0\n",
    "            b2_grad = np.sum(h2_grad, axis=0)\n",
    "            w2_grad = h1.T @ h2_grad\n",
    "            # h1_grad = 1 - np.power(np.tanh(h2_grad @ w2.T), 2) # Change this\n",
    "            h1_grad = h2_grad @ w2.T\n",
    "            h1_grad[h1 <= 0] = 0\n",
    "            b1_grad = np.sum(h1_grad, axis=0)\n",
    "            w1_grad = batch_xs.T @ h1_grad\n",
    "\n",
    "            # d/dx tanh(x) = 1 - tanh^2(x)\n",
    "\n",
    "            # if i == 0 and j == 0:\n",
    "            #     print(\"Batch shape: \", batch_xs.shape)\n",
    "            #     print(\"Preds shape: \", preds.shape)\n",
    "            #     print(\"One hot batch shape: \", batch_one_hot.shape)\n",
    "            #     print(\"Logit shape:  | Logit grad shape: \", logits.shape, logit_grad.shape)\n",
    "            #     print(\"H1 shape: | H1 grad shape: \", h1.shape, h1_grad.shape)\n",
    "            #     print(\"H2 shape: | H2 grad shape: \", h2.shape, h2_grad.shape)\n",
    "            #     print(\"W3 shape: | W3 grad shape: \", w3.shape, w3_grad.shape)\n",
    "            #     print(\"W2 shape: | W2 grad shape: \", w2.shape, w2_grad.shape)\n",
    "            #     print(\"W1 shape: | W1 grad shape: \", w1.shape, w1_grad.shape)\n",
    "            #     print(\"B3 shape: | B3 grad shape: \", b3.shape, b3_grad.shape)\n",
    "            #     print(\"B2 shape: | B2 grad shape: \", b2.shape, b2_grad.shape)\n",
    "            #     print(\"B1 shape: | B1 grad shape: \", b1.shape, b1_grad.shape)\n",
    "\n",
    "            # Applying L2 regularization\n",
    "            w1_grad += reg * np.power(w1, 2)\n",
    "            b1_grad += reg * np.power(b1, 2)\n",
    "            h1_grad += reg * np.power(h1, 2)\n",
    "            w2_grad += reg * np.power(w2, 2)\n",
    "            b2_grad += reg * np.power(b2, 2)\n",
    "            h2_grad += reg * np.power(h2, 2)\n",
    "            w3_grad += reg * np.power(w3, 2)\n",
    "            b3_grad += reg * np.power(b3, 2)\n",
    "\n",
    "            # Modifying our parameters\n",
    "            w1 -= lr * w1_grad\n",
    "            b1 -= lr * b1_grad\n",
    "            h1 -= lr * h1_grad\n",
    "            w2 -= lr * w2_grad\n",
    "            b2 -= lr * b2_grad\n",
    "            h2 -= lr * h2_grad\n",
    "            w3 -= lr * w3_grad\n",
    "            b3 -= lr * b3_grad\n",
    "           \n",
    "        # Testing our validation accuracy\n",
    "        h1 = np.maximum(valid_xs @ w1 + b1, 0)\n",
    "        h2 = np.maximum(h1 @ w2 + b2, 0)    \n",
    "        # h1 = np.tanh(valid_xs @ w1 + b1)\n",
    "        # h2 = np.tanh(h1 @ w2 + b2)\n",
    "        logits = (h2 @ w3 + b3).squeeze()\n",
    "        preds = 1 / (1 + np.exp(-logits))\n",
    "        preds_max = np.argmax(preds, axis=1)\n",
    "        accuracy = np.mean(preds_max == valid_ys)\n",
    "        print(\"Epoch:\", i, \"Validation accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79035efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Validation accuracy: 0.3967\n",
      "Epoch: 1 Validation accuracy: 0.4327\n",
      "Epoch: 2 Validation accuracy: 0.4495\n",
      "Epoch: 3 Validation accuracy: 0.4608\n",
      "Epoch: 4 Validation accuracy: 0.4761\n",
      "Epoch: 5 Validation accuracy: 0.4814\n",
      "Epoch: 6 Validation accuracy: 0.4829\n",
      "Epoch: 7 Validation accuracy: 0.4948\n",
      "Epoch: 8 Validation accuracy: 0.4984\n",
      "Epoch: 9 Validation accuracy: 0.5071\n"
     ]
    }
   ],
   "source": [
    "# Separating our data into a validation set and a training set\n",
    "train_size = int(0.8 * norm_images.shape[0])\n",
    "train_data = norm_images[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "valid_data = norm_images[train_size:]\n",
    "valid_labels = labels[train_size:]\n",
    "\n",
    "# Training our model\n",
    "train_relu(train_data.reshape(-1, 3*32*32), \n",
    "      train_labels, \n",
    "      valid_data.reshape(-1, 3*32*32),\n",
    "      valid_labels,\n",
    "      10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci378-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
