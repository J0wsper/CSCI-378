{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f56eee7-fc6d-4e47-b6da-341ccb8bfb1d",
   "metadata": {},
   "source": [
    "# Bias and Constrained Learning Homework\n",
    "\n",
    "In this homework we'll extend the constrained learning framework we used for mitigating bias in class to handle more complex situations. Specifically, we'll look at the case where the output prediction is not binary. As usual with these homeworks, there are three different levels which build on each other, each one corresponding to an increasing grade:\n",
    "\n",
    "- The basic version of this homework involves implementing code to measure fairness over multiclass classification then measuring the results when using training a regular, unfair classifier. This version is good for a C.\n",
    "- The B version of the homework involves training a classifier with some fairness constraints.\n",
    "- For an A, we'll look at slightly more complicated approach to fair training.\n",
    "\n",
    "First, we'll generate a dataset for which the sensitive attribute is binary and the output is multiclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1625,
   "id": "a7fe355b-12d1-4afa-bd3a-87bc378c0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# torch.set_printoptions(profile=\"full\")\n",
    "# np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "id": "20455201-ce05-4a10-85ce-18ba92c47fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classes = 5\n",
    "\n",
    "def generate_data():\n",
    "\n",
    "    dataset_size = 10000\n",
    "    dimensions = 40\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    A = np.concatenate((np.zeros(dataset_size // 2), np.ones(dataset_size // 2)))\n",
    "    rng.shuffle(A)\n",
    "    X = rng.normal(loc=A[:,np.newaxis], scale=1, size=(dataset_size, dimensions))\n",
    "    random_linear = np.array([\n",
    "        -2.28156561, 0.24582547, -2.48926942, -0.02934924, 5.21382855, -1.08613209,\n",
    "        2.51051602, 1.00773587, -2.10409448, 1.94385103, 0.76013416, -2.94430782,\n",
    "        0.3289264, -4.35145624, 1.61342623, -1.28433588, -2.07859612, -1.53812125,\n",
    "        0.51412713, -1.34310334, 4.67174476, 1.67269946, -2.07805413, 3.46667731,\n",
    "        2.61486654, 1.75418209, -0.06773796, 0.7213423, 2.43896438, 1.79306807,\n",
    "        -0.74610264, 2.84046827,  1.28779878, 1.84490263, 1.6949681, 0.05814582,\n",
    "        1.30510732, -0.92332861,  3.00192177, -1.76077192\n",
    "    ])\n",
    "    good_score = (X @ random_linear) ** 2 / 2\n",
    "    qs = np.quantile(good_score, (np.array(range(1, output_classes))) / output_classes)\n",
    "    Y = np.digitize(good_score, qs)\n",
    "\n",
    "    return X, A, Y\n",
    "\n",
    "X, A, Y = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "id": "42f8bebd-2e8a-49a4-86d8-995991314ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [np.int64(2000), np.int64(2000), np.int64(2000), np.int64(2000), np.int64(2000)]\n",
      "A=0: [np.int64(1385), np.int64(1308), np.int64(1106), np.int64(818), np.int64(383)]\n",
      "A=1: [np.int64(615), np.int64(692), np.int64(894), np.int64(1182), np.int64(1617)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total:\", [(Y == k).sum() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((Y == k) & (A == 0)).sum() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((Y == k) & (A == 1)).sum() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1463c-e7b1-411e-b2da-c68a54791391",
   "metadata": {},
   "source": [
    "This last cell shows the total number of data points in each output category (it should be 2000 each) as well as a breakdown of each output category for the $A=0$ group and the $A=1$ group. Note that the $A=1$ group is much more likely to be assigned to the categories with higher index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d568875-652b-42eb-8318-b4d1ecf8b51c",
   "metadata": {},
   "source": [
    "## Fairness Definition (C)\n",
    "\n",
    "Let's write some code to measure a few different forms of bias in our classifier. Demographic parity, which requires $P(R = r \\mid A = 0) = P(R = r \\mid A = 1)$ for all possible output classes $0 \\le r < K$, and predictive parity which requires $P(Y=r \\mid A = 0, R = r) = P(Y=r \\mid A = 1, R = r)$. In the the functions below,\n",
    "\n",
    "- `R` is a matrix where each row represents a probability distribution over the classes `0` to `K - 1`. That is, `R` is the output of our neural network _after_ a softmax layer.\n",
    "- `A` is a vector of sensitive attributes. Each element is either `0` or `1`.\n",
    "- `Y` is a vector of measured output classes, each element is between `0` and `K - 1`.\n",
    "\n",
    "These functions should return an array of length `K` where each element of the array represents a measure of bias for _one_ of the output classes. For example, for demographic parity, the value in the output array at index `i` should be $P(R = i \\mid A = 1) - P(R = i \\mid A = 0)$.\n",
    "\n",
    "Note that predictive parity is a bit different than the equalized odds measure I included in the solution to the bias lab. In particular, in the lab we used filtering to represent conditional probabilities, so $P(R=1 \\mid A=0)$ was measured by `probs[A==0].mean()` for example. Now we can't do that directly since the predictive parity expression is conditioned on $R$ which is continuous. You'll need to instead use Bayes' rule and/or the definition of conditional probability to rearrange the predicitive parity equation until it's something we can measure. It's quite tricky to do this for all classes in one call, so it's okay to loop over the classes and compute the predictive parity for each on separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b5c8a-20e5-4310-8277-c32134a01afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTIBUTORS: I helped and received help from Patrick Norton.\n",
    "\n",
    "def demographic_parity(R, A):\n",
    "    \n",
    "    A0 = torch.from_numpy(A == 0)\n",
    "    A1 = torch.from_numpy(A == 1)\n",
    "\n",
    "    return R[A1].mean(dim=0) - R[A0].mean(dim=0)\n",
    "\n",
    "# Bayes theorem is P(A | B) = (P(A) * P(B | A))/P(B)\n",
    "# This looks like P(Y = r | A = 0, R = r) = (P(Y = r) * P(A = 0 \\cap R = r | Y = r))/(P(A = 0 \\cap R = r))\n",
    "# We have that P(Y = r) is just the proportion of P(Y = r) to the number of data points because they're uniform and i.i.d\n",
    "\n",
    "# NOTE: Do not use this to calculate loss. Demographic parity is fine because it has no\n",
    "# explicit loops, but this one is not. The computation graph will get mangled.\n",
    "# I tried implementing this without explicit loops and I couldn't get it working.\n",
    "def predictive_parity(R, A, Y):\n",
    "    \n",
    "    # This isn't strictly necessary because P(Y = r) = 0.2 for all r\n",
    "    # However, this is a generally-extensible approach.\n",
    "    _labels, count = np.unique(Y, return_counts=True)\n",
    "    prob_y = torch.tensor(count).float().softmax(dim=0)\n",
    "\n",
    "    # Getting two of our conditions as torch tensors\n",
    "    # We need to expand dims to get the broadcasting semantics to work\n",
    "    A0 = torch.from_numpy(A == 0).long()\n",
    "    A1 = torch.from_numpy(A == 1).long()\n",
    "\n",
    "    # Getting the denominator\n",
    "    prob_ra0 = torch.tensor([(R[:,i] * A0).mean() for i in range(0, output_classes)])\n",
    "    prob_ra1 = torch.tensor([(R[:,i] * A1).mean() for i in range(0, output_classes)])\n",
    "\n",
    "    # Reversing the condition for Bayes theorem\n",
    "    prob_ray0 = torch.tensor([(R[:,i] * A0)[Y == i].mean() for i in range(0, output_classes)])\n",
    "    prob_ray1 = torch.tensor([(R[:,i] * A1)[Y == i].mean() for i in range(0, output_classes)])\n",
    "    \n",
    "    # Applying Bayes theorem\n",
    "    prob_bay_a0 = (prob_y*prob_ray0)/prob_ra0\n",
    "    prob_bay_a1 = (prob_y*prob_ray1)/prob_ra1\n",
    "\n",
    "    return prob_bay_a1 - prob_bay_a0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af3e86-3594-482d-9432-2ec2f87fa4ee",
   "metadata": {},
   "source": [
    "Now we'll train a classifier on this dataset without any fairness constraints for comparison. This code is already complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "id": "09022f4a-c7d2-4023-a736-fbc950b79cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(40, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1630,
   "id": "d3ec610e-dc36-4f79-bbe7-df81ee95042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unfair(lr=1e-1, epochs=200):\n",
    "    \n",
    "    network = MLP()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        preds = network(data_in)\n",
    "        loss_val = loss(preds, data_out)\n",
    "        opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            probs = nn.functional.softmax(preds, dim=1)\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias:\", demographic_parity(probs, A))\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1631,
   "id": "cb30fb62-4351-4575-b784-6fa621c0d6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 Accuracy: 0.3684000074863434 Bias: tensor([-0.1424, -0.1156, -0.0387,  0.0700,  0.2267], grad_fn=<SubBackward0>)\n",
      "Epoch: 199 Accuracy: 0.4424999952316284 Bias: tensor([-0.1479, -0.1202, -0.0407,  0.0713,  0.2375], grad_fn=<SubBackward0>)\n",
      "Epoch: 299 Accuracy: 0.49559998512268066 Bias: tensor([-0.1496, -0.1217, -0.0408,  0.0723,  0.2397], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = train_unfair(lr=5e-2, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "id": "5c7da734-6f06-4c3d-9e43-d508749840d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [2914, 1709, 1170, 1781, 2426]\n",
      "A=0: [2347, 1363, 436, 456, 398]\n",
      "A=1: [567, 346, 734, 1325, 2028]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114600/1547073420.py:3: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
      "/tmp/ipykernel_114600/1547073420.py:4: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])\n"
     ]
    }
   ],
   "source": [
    "p = model(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83fff5-e631-47a9-b360-0f5f69b06949",
   "metadata": {},
   "source": [
    "This classifier is probably not going to be _extremely_ accurate, but you should be able to see the bias from the dataset reflected here. Let's also measure the bias using your two functions from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "id": "20eeca88-584f-4a51-b7e9-331bb497d850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity:  tensor([-0.1496, -0.1217, -0.0408,  0.0723,  0.2398], grad_fn=<SubBackward0>)\n",
      "Predictive parity:  tensor([-0.0053, -0.0230,  0.0295,  0.1400,  0.4467])\n"
     ]
    }
   ],
   "source": [
    "p = torch.nn.functional.softmax(model(torch.tensor(X).float()), dim=1)\n",
    "print(\"Demographic parity: \", demographic_parity(p, A))\n",
    "print(\"Predictive parity: \", predictive_parity(p, A, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f3493-7763-4428-b0fb-7176a4fc1102",
   "metadata": {},
   "source": [
    "## Fair Training (B)\n",
    "\n",
    "Now we'll extend our fair training approach from the lab to the multiclass setting. Now since we have a bias measure for _each_ possible output class, we essentially have `output_classes` constraints that we need to satisfy. We can handle this within our Lagrange multiplier framework by simply adding extra multipliers for each constraint. That is, our new learning problem is\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\sum_i \\lambda_i g_i(\\beta) \\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\sum_i \\lambda_i \\left ( P_\\beta [ R = i \\mid A = 1 ] - P_\\beta [ R = i \\mid A = 0 ] \\right ) \\right )\n",
    "$$\n",
    "\n",
    "Our `demographic_parity` function gives us a vector representing $g_i(\\beta)$, so now all we need to do is replace our single parameter $\\lambda$ from the lab with a vector then compute the dot product of $\\lambda$ with our demographic parity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "id": "2d5167ac-47b9-459a-8866-a22f9167b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fair(lr=1e-1, lam_lr=1, epochs=200):\n",
    "    \n",
    "    network = MLP()\n",
    "    lam = nn.Parameter(torch.zeros(output_classes))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    lam_opt = optim.SGD([lam], lr=lam_lr, maximize=True)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Compute the loss value as defined in the Lagrangian above\n",
    "        preds = network(data_in)\n",
    "        loss_val = loss(preds, data_out)\n",
    "        probs = nn.functional.softmax(preds, dim=1)\n",
    "        bias = demographic_parity(probs, A)\n",
    "        loss_val += lam.dot(bias)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        lam_opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        lam_opt.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            probs = nn.functional.softmax(preds, dim=1)\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias:\", demographic_parity(probs, A), \"Lambda:\", lam.max().item())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "id": "f5523b6c-1990-4468-9d6b-fd06603cce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 Accuracy: 0.41119998693466187 Bias: tensor([ 0.0425,  0.0285,  0.0051, -0.0022, -0.0739], grad_fn=<SubBackward0>) Lambda: 0.5786921381950378\n",
      "Epoch: 199 Accuracy: 0.5008000135421753 Bias: tensor([ 0.0141,  0.0190,  0.0086,  0.0223, -0.0641], grad_fn=<SubBackward0>) Lambda: 0.9252098202705383\n",
      "Epoch: 299 Accuracy: 0.5562999844551086 Bias: tensor([-0.0205,  0.0298,  0.0340,  0.0018, -0.0451], grad_fn=<SubBackward0>) Lambda: 1.0653448104858398\n"
     ]
    }
   ],
   "source": [
    "model = train_fair(lr=5e-1, lam_lr=3e-1, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1636,
   "id": "04b6c773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [2052, 1918, 2147, 154, 3729]\n",
      "A=0: [1187, 1015, 947, 97, 1754]\n",
      "A=1: [865, 903, 1200, 57, 1975]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114600/1547073420.py:3: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
      "/tmp/ipykernel_114600/1547073420.py:4: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])\n"
     ]
    }
   ],
   "source": [
    "p = model(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68a6bf-043f-42db-9fdc-f23fac55129f",
   "metadata": {},
   "source": [
    "## Fair Training via KL-Divergence (A)\n",
    "\n",
    "Let's look back at our definition of demographic parity for the multiclass setting: $P(R = r \\mid A = 0) = P(R = r \\mid A = 1)$ for all possible output classes $r$. we could also express this by asserting $P(\\cdot \\mid A = 0)$ and $P(\\cdot \\mid A = 1)$ should be identical probability distributions. A natural measure of bias then would be to compute the KL-divergence between these two distributions, since KL-divergence is a measure of how \"different\" two distributions are. That is, we'll now solve the problem\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\lambda D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) \\right )\n",
    "$$\n",
    "\n",
    "However, this introduces a new complication. The KL-divergence is never negative and can only be zero if the two distributions are identical (we proved this in our first homework of the semester). That means there's no way for $\\lambda$ to ever decrease, and it will just go up forever. We can solve this by allowing a small deviation in our constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\arg\\min_\\beta &\\ L(\\beta) \\\\\n",
    "\\text{s.t.} &\\ D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) \\le \\epsilon\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can still represent this using a Lagrange multiplier:\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_{\\lambda \\ge 0} \\left ( L(\\beta) + \\lambda \\left ( D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) - \\epsilon \\right ) \\right )\n",
    "$$\n",
    "\n",
    "Your task now is to represent this optimization problem in the code below. I've taken care of clipping $\\lambda$ to zero for you since it's not something we've looked at in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4aef62-e8af-42b3-9662-1057ee4e3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kl(lr=1e-1, lam_lr=1, epochs=300, epsilon=0.1):\n",
    "    \n",
    "    network = MLP()\n",
    "    lam = nn.Parameter(torch.tensor(0.0))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    lam_opt = optim.SGD([lam], lr=lam_lr, maximize=True)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "\n",
    "    A0 = torch.from_numpy(A == 0)\n",
    "    A1 = torch.from_numpy(A == 1)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        # KL divergence is formally:\n",
    "        # DKL(P || Q) = \\sum_i P(R = i | A = 0)\\log(P(R = i | A = 0)/P(R = i | A = 1))\n",
    "        # where we sum over our output classes\n",
    "\n",
    "        # Implement the loss function above here.\n",
    "        preds = network(data_in)\n",
    "        loss_val = loss(preds, data_out)\n",
    "        probs = nn.functional.softmax(preds, dim=1)\n",
    "\n",
    "        # This formulation allows us to maintain the computation graph\n",
    "        probs_ra0 = probs[A0].mean(dim=0)\n",
    "        probs_ra1 = probs[A1].mean(dim=0)\n",
    "        log_probs = torch.log(probs_ra0/probs_ra1)\n",
    "        kl_div = (probs_ra0 * log_probs).sum()\n",
    "        loss_val += lam * (kl_div-epsilon)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        lam_opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        lam_opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lam.clamp_(min=0)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias: \", demographic_parity(probs, A), \"Divergence:\", kl_div.item(), \"Lambda:\", lam.item())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1638,
   "id": "7f7eb37f-e90b-42fa-b036-be75060738e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 Accuracy: 0.48750001192092896 Bias:  tensor([-0.0760, -0.0635, -0.0291,  0.0298,  0.1388], grad_fn=<SubBackward0>) Divergence: 0.07084843516349792 Lambda: 2.327665328979492\n",
      "Epoch: 199 Accuracy: 0.6399000287055969 Bias:  tensor([-0.0586, -0.0504, -0.0279, -0.0155,  0.1525], grad_fn=<SubBackward0>) Divergence: 0.061314746737480164 Lambda: 5.007400035858154\n",
      "Epoch: 299 Accuracy: 0.666100025177002 Bias:  tensor([-0.0629, -0.0614, -0.0025,  0.0010,  0.1258], grad_fn=<SubBackward0>) Divergence: 0.05172822251915932 Lambda: 7.014566421508789\n"
     ]
    }
   ],
   "source": [
    "model = train_kl(lr=3e-1, lam_lr=1, epsilon=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1640,
   "id": "18c4e79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfair: \n",
      "Epoch: 99 Accuracy: 0.4462999999523163 Bias: tensor([-0.1833, -0.1541, -0.0707,  0.0443,  0.3638], grad_fn=<SubBackward0>)\n",
      "Epoch: 199 Accuracy: 0.6732000112533569 Bias: tensor([-0.1793, -0.1489, -0.0597,  0.0285,  0.3594], grad_fn=<SubBackward0>)\n",
      "Epoch: 299 Accuracy: 0.7843999862670898 Bias: tensor([-0.1751, -0.1395, -0.0534,  0.0234,  0.3446], grad_fn=<SubBackward0>)\n",
      "Fair: \n",
      "Epoch: 99 Accuracy: 0.34880000352859497 Bias: tensor([ 0.0305,  0.0232,  0.0130,  0.0072, -0.0739], grad_fn=<SubBackward0>) Lambda: 0.46702802181243896\n",
      "Epoch: 199 Accuracy: 0.47200000286102295 Bias: tensor([ 0.0305,  0.0162, -0.0002,  0.0232, -0.0697], grad_fn=<SubBackward0>) Lambda: 0.7088242769241333\n",
      "Epoch: 299 Accuracy: 0.5074999928474426 Bias: tensor([ 0.0231,  0.0093, -0.0030,  0.0320, -0.0614], grad_fn=<SubBackward0>) Lambda: 0.9131637215614319\n",
      "KL: \n",
      "Epoch: 99 Accuracy: 0.446399986743927 Bias:  tensor([-0.1226, -0.1040, -0.0517,  0.0325,  0.2457], grad_fn=<SubBackward0>) Divergence: 0.20255373418331146 Lambda: 0.47169163823127747\n",
      "Epoch: 199 Accuracy: 0.6947000026702881 Bias:  tensor([-0.1275, -0.1007, -0.0360,  0.0057,  0.2585], grad_fn=<SubBackward0>) Divergence: 0.19772842526435852 Lambda: 0.6198485493659973\n",
      "Epoch: 299 Accuracy: 0.7817999720573425 Bias:  tensor([-0.1286, -0.0921, -0.0256, -0.0005,  0.2469], grad_fn=<SubBackward0>) Divergence: 0.18107280135154724 Lambda: 0.8854819536209106\n"
     ]
    }
   ],
   "source": [
    "print(\"Unfair: \")\n",
    "model1 = train_unfair(epochs=300, lr=3e-1)\n",
    "print(\"Fair: \")\n",
    "model2 = train_fair(epochs=300, lr=3e-1, lam_lr=3e-1)\n",
    "print(\"KL: \")\n",
    "model3 = train_kl(epochs=300, lr=3e-1, lam_lr=3e-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci378-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
