{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f56eee7-fc6d-4e47-b6da-341ccb8bfb1d",
   "metadata": {},
   "source": [
    "# Bias and Constrained Learning Homework\n",
    "\n",
    "In this homework we'll extend the constrained learning framework we used for mitigating bias in class to handle more complex situations. Specifically, we'll look at the case where the output prediction is not binary. As usual with these homeworks, there are three different levels which build on each other, each one corresponding to an increasing grade:\n",
    "\n",
    "- The basic version of this homework involves implementing code to measure fairness over multiclass classification then measuring the results when using training a regular, unfair classifier. This version is good for a C.\n",
    "- The B version of the homework involves training a classifier with some fairness constraints.\n",
    "- For an A, we'll look at slightly more complicated approach to fair training.\n",
    "\n",
    "First, we'll generate a dataset for which the sensitive attribute is binary and the output is multiclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "a7fe355b-12d1-4afa-bd3a-87bc378c0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "20455201-ce05-4a10-85ce-18ba92c47fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classes = 5\n",
    "\n",
    "def generate_data():\n",
    "\n",
    "    dataset_size = 10000\n",
    "    dimensions = 40\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    A = np.concatenate((np.zeros(dataset_size // 2), np.ones(dataset_size // 2)))\n",
    "    rng.shuffle(A)\n",
    "    X = rng.normal(loc=A[:,np.newaxis], scale=1, size=(dataset_size, dimensions))\n",
    "    random_linear = np.array([\n",
    "        -2.28156561, 0.24582547, -2.48926942, -0.02934924, 5.21382855, -1.08613209,\n",
    "        2.51051602, 1.00773587, -2.10409448, 1.94385103, 0.76013416, -2.94430782,\n",
    "        0.3289264, -4.35145624, 1.61342623, -1.28433588, -2.07859612, -1.53812125,\n",
    "        0.51412713, -1.34310334, 4.67174476, 1.67269946, -2.07805413, 3.46667731,\n",
    "        2.61486654, 1.75418209, -0.06773796, 0.7213423, 2.43896438, 1.79306807,\n",
    "        -0.74610264, 2.84046827,  1.28779878, 1.84490263, 1.6949681, 0.05814582,\n",
    "        1.30510732, -0.92332861,  3.00192177, -1.76077192\n",
    "    ])\n",
    "    good_score = (X @ random_linear) ** 2 / 2\n",
    "    qs = np.quantile(good_score, (np.array(range(1, output_classes))) / output_classes)\n",
    "    Y = np.digitize(good_score, qs)\n",
    "\n",
    "    return X, A, Y\n",
    "\n",
    "X, A, Y = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "42f8bebd-2e8a-49a4-86d8-995991314ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [np.int64(2000), np.int64(2000), np.int64(2000), np.int64(2000), np.int64(2000)]\n",
      "A=0: [np.int64(1415), np.int64(1342), np.int64(1110), np.int64(808), np.int64(325)]\n",
      "A=1: [np.int64(585), np.int64(658), np.int64(890), np.int64(1192), np.int64(1675)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total:\", [(Y == k).sum() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((Y == k) & (A == 0)).sum() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((Y == k) & (A == 1)).sum() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1463c-e7b1-411e-b2da-c68a54791391",
   "metadata": {},
   "source": [
    "This last cell shows the total number of data points in each output category (it should be 2000 each) as well as a breakdown of each output category for the $A=0$ group and the $A=1$ group. Note that the $A=1$ group is much more likely to be assigned to the categories with higher index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d568875-652b-42eb-8318-b4d1ecf8b51c",
   "metadata": {},
   "source": [
    "## Fairness Definition (C)\n",
    "\n",
    "Let's write some code to measure a few different forms of bias in our classifier. Demographic parity, which requires $P(R = r \\mid A = 0) = P(R = r \\mid A = 1)$ for all possible output classes $0 \\le r < K$, and predictive parity which requires $P(Y=r \\mid A = 0, R = r) = P(Y=r \\mid A = 1, R = r)$. In the the functions below,\n",
    "\n",
    "- `R` is a matrix where each row represents a probability distribution over the classes `0` to `K - 1`. That is, `R` is the output of our neural network _after_ a softmax layer.\n",
    "- `A` is a vector of sensitive attributes. Each element is either `0` or `1`.\n",
    "- `Y` is a vector of measured output classes, each element is between `0` and `K - 1`.\n",
    "\n",
    "These functions should return an array of length `K` where each element of the array represents a measure of bias for _one_ of the output classes. For example, for demographic parity, the value in the output array at index `i` should be $P(R = i \\mid A = 1) - P(R = i \\mid A = 0)$.\n",
    "\n",
    "Note that predictive parity is a bit different than the equalized odds measure I included in the solution to the bias lab. In particular, in the lab we used filtering to represent conditional probabilities, so $P(R=1 \\mid A=0)$ was measured by `probs[A==0].mean()` for example. Now we can't do that directly since the predictive parity expression is conditioned on $R$ which is continuous. You'll need to instead use Bayes' rule and/or the definition of conditional probability to rearrange the predicitive parity equation until it's something we can measure. It's quite tricky to do this for all classes in one call, so it's okay to loop over the classes and compute the predictive parity for each on separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "id": "1e4b5c8a-20e5-4310-8277-c32134a01afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTIBUTORS: I helped and received help from Patrick Norton.\n",
    "\n",
    "def demographic_parity(R, A):\n",
    "    return torch.tensor([(R[A == 1, i].mean() - R[A == 0, i].mean()) for i in range(0, output_classes)])\n",
    "\n",
    "# Bayes theorem is P(A | B) = (P(A) * P(B | A))/P(B)\n",
    "# This looks like P(Y = r | A = 0, R = r) = (P(Y = r) * P(A = 0 \\cap R = r | Y = r))/(P(A = 0 \\cap R = r))\n",
    "# We have that P(Y = r) is just the proportion of P(Y = r) to the number of data points because they're uniform and i.i.d\n",
    "def predictive_parity(R, A, Y):\n",
    "    \n",
    "    _labels, count = np.unique(Y, return_counts=True)\n",
    "    prob_y = torch.tensor(count).float().softmax(dim=0)\n",
    "\n",
    "    A0 = torch.from_numpy(A == 0).long()\n",
    "    A1 = torch.from_numpy(A == 1).long()\n",
    "\n",
    "    # Getting the denominator\n",
    "    prob_ra0 = torch.tensor([((R[:,i] * A0).mean()) for i in range(0, output_classes)])\n",
    "    prob_ra1 = torch.tensor([((R[:,i] * A1).mean()) for i in range(0, output_classes)])\n",
    "\n",
    "    # Reversing the condition\n",
    "    prob_ray0 = torch.tensor([(((R[:,i]) * A0)[Y == i].mean()) for i in range(0, output_classes)])\n",
    "    prob_ray1 = torch.tensor([(((R[:,i]) * A1)[Y == i].mean()) for i in range(0, output_classes)])\n",
    "\n",
    "    return prob_y*((prob_ray1/prob_ra1) - (prob_ray0/prob_ra0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af3e86-3594-482d-9432-2ec2f87fa4ee",
   "metadata": {},
   "source": [
    "Now we'll train a classifier on this dataset without any fairness constraints for comparison. This code is already complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "09022f4a-c7d2-4023-a736-fbc950b79cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(40, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "d3ec610e-dc36-4f79-bbe7-df81ee95042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unfair(lr=1e-1, epochs=200):\n",
    "    \n",
    "    network = MLP()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        preds = network(data_in)\n",
    "        loss_val = loss(preds, data_out)\n",
    "        opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            probs = nn.functional.softmax(preds, dim=1)\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias:\", predictive_parity(probs, A, Y))\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "cb30fb62-4351-4575-b784-6fa621c0d6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Accuracy: 0.33820000290870667 Bias: tensor([-0.0256, -0.0402,  0.0062,  0.0918,  0.3568])\n",
      "Epoch: 39 Accuracy: 0.3564999997615814 Bias: tensor([0.0539, 0.0035, 0.0259, 0.0978, 0.3369])\n",
      "Epoch: 59 Accuracy: 0.40130001306533813 Bias: tensor([0.1031, 0.0187, 0.0334, 0.0896, 0.3221])\n",
      "Epoch: 79 Accuracy: 0.45339998602867126 Bias: tensor([0.1296, 0.0243, 0.0370, 0.0746, 0.3119])\n",
      "Epoch: 99 Accuracy: 0.5105999708175659 Bias: tensor([0.1154, 0.0247, 0.0351, 0.0465, 0.3036])\n",
      "Epoch: 119 Accuracy: 0.5719000101089478 Bias: tensor([0.0858, 0.0141, 0.0210, 0.0084, 0.2761])\n",
      "Epoch: 139 Accuracy: 0.6272000074386597 Bias: tensor([ 0.0661, -0.0074,  0.0008, -0.0254,  0.2411])\n",
      "Epoch: 159 Accuracy: 0.6656000018119812 Bias: tensor([ 0.0519, -0.0297, -0.0130, -0.0459,  0.2217])\n",
      "Epoch: 179 Accuracy: 0.6960999965667725 Bias: tensor([ 0.0378, -0.0477, -0.0181, -0.0563,  0.2096])\n",
      "Epoch: 199 Accuracy: 0.7623000144958496 Bias: tensor([-0.0316, -0.0584,  0.0091, -0.0107,  0.2141])\n",
      "Epoch: 219 Accuracy: 0.8009999990463257 Bias: tensor([-0.0673, -0.0825, -0.0095,  0.0240,  0.2262])\n",
      "Epoch: 239 Accuracy: 0.7827000021934509 Bias: tensor([-0.0791, -0.0932,  0.0068, -0.0138,  0.2023])\n",
      "Epoch: 259 Accuracy: 0.7962999939918518 Bias: tensor([-0.0953, -0.1088,  0.0021, -0.0080,  0.2003])\n",
      "Epoch: 279 Accuracy: 0.7912999987602234 Bias: tensor([-0.1015, -0.1165,  0.0132, -0.0398,  0.1838])\n",
      "Epoch: 299 Accuracy: 0.8047000169754028 Bias: tensor([-0.0453, -0.1084, -0.0084, -0.0658,  0.1654])\n"
     ]
    }
   ],
   "source": [
    "model = train_unfair(lr=5e-1, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "5c7da734-6f06-4c3d-9e43-d508749840d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [2208, 2013, 1563, 3229, 987]\n",
      "A=0: [1467, 1402, 801, 1165, 165]\n",
      "A=1: [741, 611, 762, 2064, 822]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114600/1547073420.py:3: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
      "/tmp/ipykernel_114600/1547073420.py:4: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])\n"
     ]
    }
   ],
   "source": [
    "p = model(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83fff5-e631-47a9-b360-0f5f69b06949",
   "metadata": {},
   "source": [
    "This classifier is probably not going to be _extremely_ accurate, but you should be able to see the bias from the dataset reflected here. Let's also measure the bias using your two functions from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "20eeca88-584f-4a51-b7e9-331bb497d850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity:  tensor([-0.1535, -0.1291, -0.0077,  0.1560,  0.1342])\n",
      "Predictive parity:  tensor([-0.1686, -0.1800, -0.1002, -0.1081,  0.1804])\n"
     ]
    }
   ],
   "source": [
    "p = torch.nn.functional.softmax(model(torch.tensor(X).float()), dim=1)\n",
    "print(\"Demographic parity: \", demographic_parity(p, A))\n",
    "print(\"Predictive parity: \", predictive_parity(p, A, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f3493-7763-4428-b0fb-7176a4fc1102",
   "metadata": {},
   "source": [
    "## Fair Training (B)\n",
    "\n",
    "Now we'll extend our fair training approach from the lab to the multiclass setting. Now since we have a bias measure for _each_ possible output class, we essentially have `output_classes` constraints that we need to satisfy. We can handle this within our Lagrange multiplier framework by simply adding extra multipliers for each constraint. That is, our new learning problem is\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\sum_i \\lambda_i g_i(\\beta) \\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\sum_i \\lambda_i \\left ( P_\\beta [ R = i \\mid A = 1 ] - P_\\beta [ R = i \\mid A = 0 ] \\right ) \\right )\n",
    "$$\n",
    "\n",
    "Our `demographic_parity` function gives us a vector representing $g_i(\\beta)$, so now all we need to do is replace our single parameter $\\lambda$ from the lab with a vector then compute the dot product of $\\lambda$ with our demographic parity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "2d5167ac-47b9-459a-8866-a22f9167b75e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (948043511.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[860], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    loss_val = ???\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train_fair(lr=1e-1, lam_lr=1, epochs=200):\n",
    "    \n",
    "    network = MLP()\n",
    "    lam = nn.Parameter(torch.zeros(output_classes))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    lam_opt = optim.SGD([lam], lr=lam_lr, maximize=True)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Compute the loss value as defined in the Lagrangian above\n",
    "        loss_val = ???\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        lam_opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        lam_opt.step()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            probs = nn.functional.softmax(preds, dim=1)\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias:\", demographic_parity(probs, A), \"Lambda:\", lam.max().item())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5523b6c-1990-4468-9d6b-fd06603cce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_fair(lr=5e-1, lam_lr=3e-1, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402424e-6193-4550-a458-5eae0b3557a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68a6bf-043f-42db-9fdc-f23fac55129f",
   "metadata": {},
   "source": [
    "## Fair Training via KL-Divergence (A)\n",
    "\n",
    "Let's look back at our definition of demographic parity for the multiclass setting: $P(R = r \\mid A = 0) = P(R = r \\mid A = 1)$ for all possible output classes $r$. we could also express this by asserting $P(\\cdot \\mid A = 0)$ and $P(\\cdot \\mid A = 1)$ should be identical probability distributions. A natural measure of bias then would be to compute the KL-divergence between these two distributions, since KL-divergence is a measure of how \"different\" two distributions are. That is, we'll now solve the problem\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\lambda D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) \\right )\n",
    "$$\n",
    "\n",
    "However, this introduces a new complication. The KL-divergence is never negative and can only be zero if the two distributions are identical (we proved this in our first homework of the semester). That means there's no way for $\\lambda$ to ever decrease, and it will just go up forever. We can solve this by allowing a small deviation in our constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\arg\\min_\\beta &\\ L(\\beta) \\\\\n",
    "\\text{s.t.} &\\ D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) \\le \\epsilon\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can still represent this using a Lagrange multiplier:\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_{\\lambda \\ge 0} \\left ( L(\\beta) + \\lambda \\left ( D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) - \\epsilon \\right ) \\right )\n",
    "$$\n",
    "\n",
    "Your task now is to represent this optimization problem in the code below. I've taken care of clipping $\\lambda$ to zero for you since it's not something we've looked at in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4aef62-e8af-42b3-9662-1057ee4e3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kl(lr=1e-1, lam_lr=1, epochs=300, epsilon=0.1):\n",
    "    \n",
    "    network = MLP()\n",
    "    lam = nn.Parameter(torch.tensor(0.0))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    lam_opt = optim.SGD([lam], lr=lam_lr, maximize=True)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Implement the loss function above here.\n",
    "        loss_val = ???\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        lam_opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        lam_opt.step()\n",
    "        with torch.no_grad():\n",
    "            lam.clamp_(min=0)\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Divergence:\", kl_div.item(), \"Lambda:\", lam.item())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7eb37f-e90b-42fa-b036-be75060738e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_kl(lr=3e-1, lam_lr=1, epsilon=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af25af0-dfe6-4474-8ebe-544359ff98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci378-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
